{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, metrics, optimizers, losses, callbacks\n",
    "\n",
    "from utils.data_loader import create_dataset\n",
    "from utils.model_loss import TverskyBCEPerSequence\n",
    "from utils.model_inference_plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/final_model_data_all_scaled.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Bx', 'By', 'Bz', 'Bx_lag_1', 'Bx_lag_2', 'By_lag_1',\n",
    "        'By_lag_2', 'Bz_lag_1', 'Bz_lag_2', 'Bx_conditional_vol',\n",
    "        'By_conditional_vol', 'Bz_conditional_vol', 'Bx_rolling_stdev',\n",
    "        'By_rolling_stdev', 'Bz_rolling_stdev']].values\n",
    "\n",
    "y = df['Event_label_80'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is split manually since it was randomly shuffled in 6-3_model_dataset.ipynb\n",
    "total_samples = len(X)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# 80% of the data used for training\n",
    "# for models with just one satellite 60% of the data was used for training\n",
    "train_size = int(0.8 * total_samples)\n",
    "test_size = total_samples - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for the model, n_timesteps is the number of timesteps in each sequence\n",
    "# 500 corresponds to ~25 minutes of data which is the average event duration\n",
    "# The stride of 40 reduces redundancy between sequences and speeds up training times\n",
    "batch_size = 256\n",
    "n_timesteps = 500\n",
    "stride = 40\n",
    "\n",
    "train_idx = (0, train_size)\n",
    "test_idx = (train_size, total_samples)\n",
    "\n",
    "# Creates the train and test datasets for the model\n",
    "train_dataset = create_dataset(X, y, n_timesteps, batch_size, stride, train_idx[0], train_idx[1])\n",
    "test_dataset = create_dataset(X, y, n_timesteps, batch_size, stride, test_idx[0], test_idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines the number of steps to train and test the model, if not defined model\n",
    "# will run indefinitely \n",
    "steps_train_epoch = int(np.ceil((train_size - n_timesteps) / (stride * batch_size)))\n",
    "steps_test_epoch = int(np.ceil((test_size - n_timesteps) / (stride * batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell determines the optimal number of batches by looking at the \n",
    "# distribution of sequences with events\n",
    "\n",
    "batch_ratios = []\n",
    "\n",
    "for _, outputs in test_dataset.take(steps_train_epoch):\n",
    "    y_time = outputs['time_output'].numpy()\n",
    "    has_event = (np.sum(y_time, axis=1) > 0).astype(np.float32)\n",
    "    \n",
    "    batch_ratios.append(np.mean(has_event))\n",
    "\n",
    "plt.hist(batch_ratios, bins=30, edgecolor='black')\n",
    "plt.xlabel(\"Percentage of 1s in Sequence\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Sequences with 1s per Batch (Train Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Input shape: (batch_size, n_timesteps, n_features)\n",
    "input_layer = layers.Input(shape=(n_timesteps, n_features))\n",
    "\n",
    "# Initial convolution to capture local patterns in the time series\n",
    "x = layers.Conv1D(kernel_size=5, filters=64, padding='same', activation='gelu')(input_layer)\n",
    "x = layers.LayerNormalization()(x)\n",
    "\n",
    "# First BiLSTM layer to capture temporal dependencies in both directions\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "\n",
    "# Self-attention layer to allow each time step to attend to others\n",
    "attention, attention_weights = layers.MultiHeadAttention(num_heads=4, key_dim=64)(x, x, return_attention_scores=True)\n",
    "\n",
    "# Residual connection + normalization\n",
    "x = layers.Add()([x, attention])\n",
    "x = layers.LayerNormalization()(x)\n",
    "\n",
    "# Second BiLSTM layer for deeper sequence modeling\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "\n",
    "# Save this intermediate output for a skip connection\n",
    "skip = x\n",
    "\n",
    "# Feedforward layers to transform representation\n",
    "x = layers.Dense(128, activation='gelu')(x)\n",
    "x = layers.Dense(64, activation='gelu')(x)\n",
    "\n",
    "# Down-project skip connection to match dimensionality\n",
    "skip = layers.Dense(64)(skip)\n",
    "\n",
    "# Concatenate skip connection with feedforward output\n",
    "x = layers.Concatenate()([x, skip])\n",
    "\n",
    "# Final transformation before output layers\n",
    "x = layers.Dense(32, activation='gelu')(x)\n",
    "\n",
    "# Time-distributed output (per time step): sigmoid for binary classification\n",
    "output_time_layer = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'), name=\"time_output\")(x)\n",
    "\n",
    "# Sequence-level output: Global average pooling over the features to capture latent patterns, then sigmoid\n",
    "# The model learns to relate different event densities to different patterns\n",
    "x_seq = layers.GlobalAveragePooling1D()(x)\n",
    "output_seq_layer = layers.Dense(1, activation='sigmoid', name=\"sequence_output\")(x_seq)\n",
    "\n",
    "# Define the full model with two outputs\n",
    "model = models.Model(inputs=input_layer, outputs=[output_time_layer, output_seq_layer])\n",
    "\n",
    "# Compile the model with custom and standard loss functions\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "    \n",
    "    # Custom Tversky + Focal BCE loss for time step predictions,\n",
    "    # and Huber loss for sequence-level summary prediction\n",
    "    loss={\n",
    "        'time_output': TverskyBCEPerSequence(\n",
    "            alpha_t=0.6,\n",
    "            beta_t=0.7,\n",
    "            alpha_f=0.25,\n",
    "            gamma_f=1.5,\n",
    "            event_weight=1.75\n",
    "        ),\n",
    "        'sequence_output': losses.Huber()\n",
    "    },\n",
    "    \n",
    "    # Equal weighting for both losses\n",
    "    loss_weights={\n",
    "        'time_output': 1.0,\n",
    "        'sequence_output': 1.0\n",
    "    },\n",
    "    \n",
    "    # Track accuracy, precision, and recall for the time step predictions\n",
    "    metrics={\n",
    "        'time_output': ['accuracy', metrics.Precision(), metrics.Recall()]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    min_lr=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=steps_train_epoch,\n",
    "    callbacks=[lr_schedule],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.utils.plot_model(\n",
    "#     model,\n",
    "#     to_file=\"model.png\",\n",
    "#     show_shapes=True,\n",
    "#     show_dtype=False,\n",
    "#     show_layer_names=False,\n",
    "#     rankdir=\"TD\",\n",
    "#     expand_nested=False,\n",
    "#     dpi=200,\n",
    "#     show_layer_activations=False,\n",
    "#     show_trainable=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/mosrl_80_all_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probas_raw = model.predict(test_dataset, steps=steps_test_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the per timestep predictions\n",
    "y_pred_probas_sqzd = y_pred_probas_raw[0].squeeze(-1)\n",
    "num_windows, window_size = y_pred_probas_sqzd.shape\n",
    "\n",
    "# Calculate the total output length to align predictions with the original time series\n",
    "# Subtracts 39 to align with y_test length (if needed). Remove \"- 39\" if not applicable.\n",
    "output_len = num_windows * stride + window_size - 39\n",
    "\n",
    "# Initialize arrays to store the sum of predictions and counts for averaging\n",
    "sum_preds = np.zeros(output_len, dtype=y_pred_probas_sqzd.dtype)\n",
    "count_preds = np.zeros(output_len, dtype=int)\n",
    "\n",
    "# Loop over each sliding window prediction\n",
    "for win_num in range(num_windows):\n",
    "    start = win_num * stride # Start index of the window in the original timeline\n",
    "    end = start + window_size # End index of the window\n",
    "    \n",
    "    # Accumulate the predictions from overlapping windows\n",
    "    sum_preds[start:end] += y_pred_probas_sqzd[win_num]\n",
    "    \n",
    "    # Count how many times each time step has been predicted\n",
    "    count_preds[start:end] += 1\n",
    "\n",
    "# Average the predictions across overlapping windows (ignoring zero divisions)\n",
    "y_pred_probas = np.divide(sum_preds, count_preds, where=count_preds != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('models/mosrl_80_all_pred_probas.npy', y_pred_probas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "space_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
